import whisper
import pyaudio
import wave
import numpy as np
import speech_recognition as sr
import torch

print(torch.cuda.is_available())
device = "cuda" if torch.cuda.is_available() else "cpu"


# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (–º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å tiny, base, small, medium, large)
model = whisper.load_model("medium").to(device)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
RATE = 16000  # –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
CHUNK = 1024  # –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –∑–∞–ø–∏—Å–∏

sr_recognizer = sr.Recognizer()
with sr.Microphone() as source:
    print("üîÑ –ü–æ–¥—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥ –æ–∫—Ä—É–∂–∞—é—â–∏–π —à—É–º...")
    sr_recognizer.adjust_for_ambient_noise(source, duration=2)  # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø–æ —à—É–º—É

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º PyAudio
p = pyaudio.PyAudio()
stream = p.open(format=pyaudio.paInt16, channels=1, rate=RATE,
                input=True, frames_per_buffer=CHUNK)
stream.start_stream()

print("üé§ –ì–æ–≤–æ—Ä–∏—Ç–µ... (—Å–∫–∞–∂–∏—Ç–µ '—Å—Ç–æ–ø' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)")

frames = []
while True:
    data = stream.read(CHUNK)
    print('data', data)
    frames.append(data)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–∞—Å—Å–∏–≤ numpy
    audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
    text = model.transcribe(audio_data, language='ru', fp16=False, verbose=True)['text']
    # text = whisper.transcribe(model, audio_data)["text"].lower()

    # mel = whisper.log_mel_spectrogram(audio_data).to(model.device)
    # _, probs = model.detect_language(mel)
    # print(f"Detected language: {max(probs, key=probs.get)}")

    print("–í—ã —Å–∫–∞–∑–∞–ª–∏:", text)
    if "—Å—Ç–æ–ø" in text:
        print("‚èπ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã")
        break  # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–≥—Ä–∞–º–º—É

    # # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–∫–∞–∑–∞–ª–∏ –ª–∏ "—Å—Ç–æ–ø"
    # if "—Å—Ç–æ–ø" in :
    #     print("‚èπ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã")
    #     break

# –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏ –∑–∞–∫—Ä—ã–≤–∞–µ–º –ø–æ—Ç–æ–∫
stream.stop_stream()
stream.close()
p.terminate()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
# with wave.open("output.wav", "wb") as wf:
#     wf.setnchannels(1)
#     wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))
#     wf.setframerate(RATE)
#     wf.writeframes(b''.join(frames))

# print("üéß –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏...")
# result = model.transcribe("output.wav")
# print("–í—ã —Å–∫–∞–∑–∞–ª–∏:", result["text"])
